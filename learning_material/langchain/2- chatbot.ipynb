{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "modified by: Chadi Abi Fadel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::note\n",
    "\n",
    "This tutorial previously used the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) abstraction. You can access that version of the documentation in the [v0.2 docs](https://python.langchain.com/v0.2/docs/tutorials/chatbot/).\n",
    "\n",
    "As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n",
    "\n",
    "If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n",
    "\n",
    "Please see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n",
    ":::\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. \n",
    "This chatbot will be able to have a conversation and remember previous interactions with a [chat model](/docs/concepts/chat_models).\n",
    "\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation.\n",
    "There are several other related concepts that you may be looking for:\n",
    "\n",
    "- [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data\n",
    "- [Agents](/docs/tutorials/agents): Build a chatbot that can take actions\n",
    "\n",
    "This tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n",
    "\n",
    "This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.\n",
    "\n",
    "### Installation\n",
    "\n",
    "For this tutorial we will need `langchain-core` and `langgraph`. This guide requires `langgraph >= 0.2.28`.\n",
    "\n",
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
    "    <CodeBlock language=\"bash\">pip install langchain-core langgraph>0.2.27</CodeBlock>\n",
    "  </TabItem>\n",
    "  <TabItem value=\"conda\" label=\"Conda\">\n",
    "    <CodeBlock language=\"bash\">conda install langchain-core langgraph>0.2.27 -c conda-forge</CodeBlock>\n",
    "  </TabItem>\n",
    "</Tabs>\n",
    "\n",
    "\n",
    "\n",
    "For more details, see our [Installation guide](/docs/how_to/installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize OpenAI's GPT-4o-mini model through LangChain\n",
    "# gpt-4o-mini: Cost-effective, fast model suitable for most conversational tasks\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use the model directly. `ChatModel`s are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Chadi! How can I assist you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Direct model invocation with a single message\n",
    "# Creates a HumanMessage object and sends it to the model\n",
    "# Returns AI response without any conversation history or context\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Chadi\")]).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct model call asking for name - will fail to remember previous interaction\n",
    "# No conversation history passed, so model doesn't know user introduced themselves as \"Bob\"\n",
    "# Demonstrates stateless behavior: each invoke() call is independent\n",
    "model.invoke([HumanMessage(content=\"What's my name?\")]).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the example [LangSmith trace](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)\n",
    "\n",
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\n",
    "This makes for a terrible chatbot experience!\n",
    "\n",
    "To get around this, we need to pass the entire [conversation history](/docs/concepts/chat_history) into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob! How can I help you today?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Manual conversation history management\n",
    "# Includes full conversation context: introduction + AI response + current question\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),                           # Original introduction\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),   # Previous AI response\n",
    "        HumanMessage(content=\"What's my name?\"),                       # Current question\n",
    "    ]\n",
    ").content\n",
    "# Model now has context and can answer \"Bob\" because full conversation is provided\n",
    "# Demonstrates stateless model with manual state management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally.\n",
    "So how do we best implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message persistence\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Create a stateful conversation workflow\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define function that processes conversation state\n",
    "def call_model(state: MessagesState):\n",
    "    # Send entire conversation history to model\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # Return response wrapped in messages format for state update\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Build workflow graph with single processing node\n",
    "workflow.add_edge(START, \"model\")        # Route from start directly to model\n",
    "workflow.add_node(\"model\", call_model)   # Define model node that runs call_model function\n",
    "\n",
    "# Add persistent memory capability\n",
    "memory = MemorySaver()                   # Creates in-memory conversation storage\n",
    "app = workflow.compile(checkpointer=memory)  # Compile workflow with memory checkpointing\n",
    "\n",
    "# Result: Stateful chatbot that automatically manages conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration for conversation thread management\n",
    "# thread_id: \"abc123\" creates unique conversation session for memory persistence\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n",
    "\n",
    "We can then invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Chadi! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Execute workflow with user introduction\n",
    "query = \"Hi! I'm Chadi.\"                           # User's greeting message\n",
    "input_messages = [HumanMessage(query)]             # Wrap in LangGraph message format\n",
    "output = app.invoke({\"messages\": input_messages}, config)  # Run workflow with thread config\n",
    "\n",
    "# Display AI's response\n",
    "output[\"messages\"][-1].pretty_print()              # Get last message (AI response) and format nicely\n",
    "# Note: output contains full conversation state with both input and AI messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Chadi. How can I help you today, Chadi?\n"
     ]
    }
   ],
   "source": [
    "# Test memory functionality - ask for previously mentioned information\n",
    "query = \"What's my name?\"                          # Question about earlier introduction\n",
    "input_messages = [HumanMessage(query)]             # Format as LangGraph message\n",
    "output = app.invoke({\"messages\": input_messages}, config)  # Use same thread_id \"abc123\"\n",
    "\n",
    "# Display AI's response - should remember \"Chadi\" from previous interaction\n",
    "output[\"messages\"][-1].pretty_print()              # AI can access stored conversation history\n",
    "# Memory allows AI to recall user introduced themselves as \"Chadi\" in this thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't know your name. If you'd like to share it, feel free!\n"
     ]
    }
   ],
   "source": [
    "# Test thread isolation - use different thread_id with same question\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}  # New thread - no previous memory\n",
    "input_messages = [HumanMessage(query)]              # Same question: \"What's my name?\"\n",
    "output = app.invoke({\"messages\": input_messages}, config)  # Invoke with new thread\n",
    "\n",
    "# AI won't know the name since this is a fresh conversation thread\n",
    "output[\"messages\"][-1].pretty_print()               # No access to \"Chadi\" from thread \"abc123\"\n",
    "# Demonstrates memory isolation: each thread_id maintains separate conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Chadi. If there's anything else you'd like to talk about, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "# Switch back to original thread - memory should be restored\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}  # Return to thread where user said \"Hi! I'm Chadi\"\n",
    "input_messages = [HumanMessage(query)]              # Same question: \"What's my name?\"\n",
    "output = app.invoke({\"messages\": input_messages}, config)  # Use original thread config\n",
    "\n",
    "# AI should remember \"Chadi\" again - memory persists across thread switches\n",
    "output[\"messages\"][-1].pretty_print()               # Access to conversation history restored\n",
    "# Demonstrates persistent memory: returning to same thread_id retrieves stored context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\n",
    "\n",
    "## Prompt templates\n",
    "\n",
    "[Prompt Templates](/docs/concepts/prompt_templates) help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
    "\n",
    "To add in a system message, we will create a `ChatPromptTemplate`. We will utilize `MessagesPlaceholder` to pass all the messages in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create structured prompt template with pirate personality\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),                                                    # System message sets AI behavior/personality\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),        # Placeholder for conversation history\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Template structure when invoked:\n",
    "# System: You talk like a pirate. Answer all questions...\n",
    "# [Full conversation history inserted here]\n",
    "# Results in pirate-themed responses while maintaining conversation context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our application to incorporate this template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create workflow with prompt templating for personality injection\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-start\n",
    "    prompt = prompt_template.invoke(state)        # Format conversation with pirate system message\n",
    "    response = model.invoke(prompt)               # Send formatted prompt (not raw messages) to model\n",
    "    # highlight-end\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Build workflow structure\n",
    "workflow.add_edge(START, \"model\")                 # Route from start to model\n",
    "workflow.add_node(\"model\", call_model)            # Define model processing node\n",
    "\n",
    "# Add persistent memory and compile\n",
    "memory = MemorySaver()                            # Memory storage for conversation history\n",
    "app = workflow.compile(checkpointer=memory)       # Compile with memory checkpointing\n",
    "\n",
    "# Result: Stateful pirate chatbot that applies personality to all responses\n",
    "# Key difference: prompt_template wraps conversation history with system instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invoke the application in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! What brings ye to these waters today? Speak up, and I'll do me best to assist ye! Arrr!\n"
     ]
    }
   ],
   "source": [
    "# Test pirate-themed workflow with user introduction\n",
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}  # New conversation thread\n",
    "query = \"Hi! I'm Jim.\"                              # User introduction\n",
    "input_messages = [HumanMessage(query)]              # Format as LangGraph message\n",
    "output = app.invoke({\"messages\": input_messages}, config)  # Run pirate workflow\n",
    "\n",
    "# Display AI's pirate-themed response to Jim's greeting\n",
    "output[\"messages\"][-1].pretty_print()               # Should respond like a pirate: \"Ahoy there, Jim!\" etc.\n",
    "# Prompt template applies pirate personality while storing conversation in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yer name be Jim, savvy? What else can I do fer ye, matey? Arrr!\n"
     ]
    }
   ],
   "source": [
    "# Test pirate workflow memory - ask for previously mentioned name\n",
    "query = \"What is my name?\"                          # Question about earlier introduction\n",
    "input_messages = [HumanMessage(query)]              # Format as LangGraph message\n",
    "output = app.invoke({\"messages\": input_messages}, config)  # Use same thread_id \"abc345\"\n",
    "\n",
    "# AI should remember \"Jim\" AND respond in pirate style\n",
    "output[\"messages\"][-1].pretty_print()               # Expected: \"Yer name be Jim, matey!\" or similar\n",
    "# Demonstrates both memory persistence and personality consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create prompt template with dynamic language support\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),                                                    # System message with language variable placeholder\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),        # Placeholder for conversation history\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Template requires both 'messages' and 'language' parameters when invoked\n",
    "# Example: prompt_template.invoke({\"messages\": [...], \"language\": \"Spanish\"})\n",
    "# Results in multilingual responses while maintaining conversation context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have added a new `language` input to the prompt. Our application now has two parameters-- the input `messages` and `language`. We should update our application's state to reflect this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# Define custom state schema extending basic MessagesState\n",
    "# highlight-next-line\n",
    "class State(TypedDict):\n",
    "    # highlight-next-line\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]  # Conversation history with message aggregation\n",
    "    # highlight-next-line\n",
    "    language: str                                             # Language preference stored in state\n",
    "\n",
    "# Create workflow with custom state schema\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)                    # Uses both messages and language from state\n",
    "    response = model.invoke(prompt)                           # Send formatted prompt to model\n",
    "    return {\"messages\": [response]}                           # Return only messages (language persists in state)\n",
    "\n",
    "# Build workflow structure\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Compile with memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Result: Stateful multilingual chatbot that remembers both conversation history and language preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "مرحبًا شادي! كيف يمكنني مساعدتك اليوم؟\n"
     ]
    }
   ],
   "source": [
    "# Execute multilingual workflow with language preference\n",
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}  # New conversation thread\n",
    "query = \"Hi! I'm chadi.\"                              # User introduction\n",
    "language = \"arabic\"                                # Language preference\n",
    "input_messages = [HumanMessage(query)]              # Format user message\n",
    "\n",
    "output = app.invoke(\n",
    "    # highlight-next-line\n",
    "    {\"messages\": input_messages, \"language\": language},  # Pass both messages AND language to state\n",
    "    config,\n",
    ")\n",
    "\n",
    "# AI responds to Bob's greeting in Spanish\n",
    "output[\"messages\"][-1].pretty_print()               # Expected: Spanish greeting like \"¡Hola Bob!\"\n",
    "# Language preference is now stored in thread state for future interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entire state is persisted, so we can omit parameters like `language` if no changes are desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "اسمك هو شادي.\n"
     ]
    }
   ],
   "source": [
    "# Test memory persistence for both name and language preference\n",
    "query = \"What is my name?\"                          # Ask for previously mentioned name\n",
    "input_messages = [HumanMessage(query)]              # Format user message\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},                    # Only pass messages - no language specified\n",
    "    config,\n",
    ")\n",
    "\n",
    "# AI should remember both \"Bob\" AND continue using Spanish from previous state\n",
    "output[\"messages\"][-1].pretty_print()               # Expected: Spanish response like \"Tu nombre es Bob\"\n",
    "# Language preference persists in thread memory even when not explicitly passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you understand what's happening internally, check out [this LangSmith trace](https://smith.langchain.com/public/15bd8589-005c-4812-b9b9-23e74ba4c3c6/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Conversation History\n",
    "\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "**Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.**\n",
    "\n",
    "We can do this by adding a simple step in front of the prompt that modifies the `messages` key appropriately, and then wrap that new chain in the Message History class. \n",
    "\n",
    "LangChain comes with a few built-in helpers for [managing a list of messages](/docs/how_to/#messages). In this case we'll use the [trim_messages](/docs/how_to/trim_messages/) helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "# Configure message trimmer for context window management\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,           # Limit conversation to 65 tokens maximum\n",
    "    strategy=\"last\",         # Keep most recent messages, discard oldest\n",
    "    token_counter=model,     # Use model's tokenizer for accurate token counting\n",
    "    include_system=True,     # Always preserve system message regardless of token limit\n",
    "    allow_partial=False,     # Don't split messages mid-sentence\n",
    "    start_on=\"human\",        # When trimming, ensure context begins with human message\n",
    ")\n",
    "\n",
    "# Example long conversation history (11 messages total)\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),  # System prompt (always kept)\n",
    "    HumanMessage(content=\"hi! I'm bob\"),               # Initial greeting\n",
    "    AIMessage(content=\"hi!\"),                          # AI response\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),  # User info\n",
    "    AIMessage(content=\"nice\"),                         # AI acknowledgment\n",
    "    HumanMessage(content=\"whats 2 + 2\"),              # Math question\n",
    "    AIMessage(content=\"4\"),                            # AI answer\n",
    "    HumanMessage(content=\"thanks\"),                    # User thanks\n",
    "    AIMessage(content=\"no problem!\"),                 # AI response\n",
    "    HumanMessage(content=\"having fun?\"),               # Recent question\n",
    "    AIMessage(content=\"yes!\"),                         # Latest response\n",
    "]\n",
    "\n",
    "# Apply trimming - returns only messages that fit within 65-token limit\n",
    "# Keeps system message + most recent exchanges, discards older messages\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  use it in our chain, we just need to run the trimmer before we pass the `messages` input to our prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create workflow with memory trimming and multilingual support\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    # highlight-start\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])      # Trim conversation to fit token limit\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}  # Format with trimmed messages + language\n",
    "    )\n",
    "    response = model.invoke(prompt)                           # Send optimized prompt to model\n",
    "    # highlight-end\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build workflow structure\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Compile with persistent memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Result: Advanced chatbot with:\n",
    "# - Persistent conversation memory across sessions\n",
    "# - Automatic context window management (token trimming)\n",
    "# - Multilingual support with language preference memory\n",
    "# - System message preservation during trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "عذراً، لا أستطيع معرفة اسمك. لكنني هنا لمساعدتك في أي شيء تحتاجه!\n"
     ]
    }
   ],
   "source": [
    "# Test trimming functionality with long conversation history\n",
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}  # New conversation thread\n",
    "query = \"What is my name?\"                          # Ask for name from conversation history\n",
    "language = \"arabic\"                                # Set response language\n",
    "\n",
    "# highlight-next-line\n",
    "input_messages = messages + [HumanMessage(query)]   # Combine full 11-message history + new question (12 total)\n",
    "\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},  # Pass long message history to workflow\n",
    "    config,\n",
    ")\n",
    "\n",
    "# AI may not remember \"bob\" if trimmer discarded early messages due to token limit\n",
    "output[\"messages\"][-1].pretty_print()               # Response depends on what trimmer kept from 65-token limit\n",
    "# Demonstrates automatic context management: older context gets trimmed to fit model limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we ask about information that is within the last few messages, it remembers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems you haven't asked a math problem yet. If you have one in mind, feel free to share it, and I'll be happy to help!\n"
     ]
    }
   ],
   "source": [
    "# Test trimming with query about mid-conversation content\n",
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}  # New conversation thread\n",
    "query = \"What math problem did I ask?\"              # Ask about math question from message history\n",
    "language = \"English\"                                # Set response language\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]   # Combine 11-message history + new question (12 total)\n",
    "\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},  # Pass long conversation to workflow\n",
    "    config,\n",
    ")\n",
    "\n",
    "# AI likely remembers \"whats 2 + 2\" since math exchange is near end of conversation\n",
    "output[\"messages\"][-1].pretty_print()               # Should recall \"2 + 2\" - kept by \"last\" trimming strategy\n",
    "# Demonstrates smart trimming: recent context (including math problem) preserved within token limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at LangSmith, you can see exactly what is happening under the hood in the [LangSmith trace](https://smith.langchain.com/public/04402eaa-29e6-4bb1-aa91-885b730b6c21/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Now we've got a functioning chatbot. However, one *really* important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\n",
    "\n",
    "It's actually super easy to do this!\n",
    "\n",
    "By default, `.stream` in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting `stream_mode=\"messages\"` allows us to stream output tokens instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Todd|!| Here's| another| joke| for| you|:\n",
      "\n",
      "|Why| don't| scientists| trust| atoms|?\n",
      "\n",
      "|Because| they| make| up| everything|!||"
     ]
    }
   ],
   "source": [
    "# Demonstrate streaming response functionality\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}  # New conversation thread\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"       # User introduction + joke request\n",
    "language = \"English\"                                # Set response language\n",
    "input_messages = [HumanMessage(query)]              # Format user message\n",
    "\n",
    "# highlight-next-line\n",
    "for chunk, metadata in app.stream(                  # Stream response in real-time chunks\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",                          # Stream individual messages as they're generated\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):                 # Filter to only AI response chunks\n",
    "        print(chunk.content, end=\"|\")                # Print each chunk with \"|\" separator\n",
    "\n",
    "# Result: Real-time streaming of AI joke response, showing text as it's generated\n",
    "# Useful for long responses to provide immediate user feedback instead of waiting for completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\n",
    "\n",
    "- [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data\n",
    "- [Agents](/docs/tutorials/agents): Build a chatbot that can take actions\n",
    "\n",
    "If you want to dive deeper on specifics, some things worth checking out are:\n",
    "\n",
    "- [Streaming](/docs/how_to/streaming): streaming is *crucial* for chat applications\n",
    "- [How to add message history](/docs/how_to/message_history): for a deeper dive into all things related to message history\n",
    "- [How to manage large message history](/docs/how_to/trim_messages/): more techniques for managing a large chat history\n",
    "- [LangGraph main docs](https://langchain-ai.github.io/langgraph/): for more detail on building with LangGraph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
